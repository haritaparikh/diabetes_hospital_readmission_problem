{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, fbeta_score, accuracy_score, f1_score, precision_score, recall_score, precision_recall_curve \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import scipy\n",
    "from sklearn.utils import resample\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification_pred_and_metric_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classification_pred_and_metric_profile:\n",
    "    '''\n",
    "    Predicts the hardclasses for classiciation model.\n",
    "    Uses the cut-off based on selection of metric.\n",
    "    Stores and saves metrics.\n",
    "    '''\n",
    "    def __init__(self, model, algorithm_name):\n",
    "        self.model = model\n",
    "        self.metric_cutoff_dict = {}\n",
    "        self.metric = {}\n",
    "        self.algorithm_name = algorithm_name\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.output_path = os.path.join(\"../output\",timestr)\n",
    "        os.makedirs(self.output_path) \n",
    "    \n",
    "    def get_predictions(self, \n",
    "                        data):\n",
    "        '''\n",
    "        Predict probabilities for data.\n",
    "        '''\n",
    "        return self.model.predict_proba(data)[:,1]\n",
    "        \n",
    "    def get_metric_cutoff(self, \n",
    "                          x_train, \n",
    "                          y_train):\n",
    "        '''\n",
    "        Estimate optimum metric cut_off for the metric of selection and save map.\n",
    "        '''\n",
    "        actual = y_train.values.ravel()\n",
    "        cutoffs = np.linspace(0.001,0.999,999)\n",
    "        fbetas=[]\n",
    "        accuracy_scores = []\n",
    "        f1_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        \n",
    "        predicted_train = self.get_predictions(x_train)\n",
    "        \n",
    "        for cutoff in cutoffs:    \n",
    "            predicted=(predicted_train>cutoff).astype(int)  \n",
    "            \n",
    "            fbetas.append(fbeta_score(actual, \n",
    "                                      predicted, \n",
    "                                      beta=2))\n",
    "            accuracy_scores.append(accuracy_score(actual, \n",
    "                                                  predicted))             \n",
    "            f1_scores.append(f1_score(actual,\n",
    "                                      predicted)) \n",
    "            precision_scores.append(precision_score(actual,\n",
    "                                                    predicted)) \n",
    "            recall_scores.append(recall_score(actual,\n",
    "                                              predicted))  \n",
    "          \n",
    "        cutoff_map = list(zip(cutoffs,\n",
    "                              fbetas))\n",
    "        with open(os.path.join(self.output_path, \n",
    "                               \"%s_fbeta_cutoff_map.json\"%self.algorithm_name), \n",
    "                  \"w\") as outfile:\n",
    "            json.dump(cutoff_map, outfile)     \n",
    "        cutoff_optimum = cutoffs[fbetas == max(fbetas)][0] \n",
    "        self.metric_cutoff_dict[\"fbetas\"] = {\"map\":cutoff_map, \n",
    "                                             \"optimum\": cutoff_optimum}\n",
    "        \n",
    "        cutoff_map = list(zip(cutoffs,accuracy_scores))\n",
    "        with open(os.path.join(self.output_path, \n",
    "                               \"%s_accuracy_cutoff_map.json\"%self.algorithm_name), \n",
    "                  \"w\") as outfile:\n",
    "            json.dump(cutoff_map, outfile)  \n",
    "        cutoff_optimum = cutoffs[accuracy_scores == max(accuracy_scores)][0] \n",
    "        self.metric_cutoff_dict[\"accuracy_scores\"] = {\"map\":cutoff_map, \n",
    "                                                      \"optimum\": cutoff_optimum} \n",
    "        \n",
    "        cutoff_map = list(zip(cutoffs,f1_scores))\n",
    "        with open(os.path.join(self.output_path, \n",
    "                               \"%s_f1_score_cutoff_map.json\"%self.algorithm_name), \n",
    "                  \"w\") as outfile:\n",
    "            json.dump(cutoff_map, outfile)  \n",
    "        cutoff_optimum = cutoffs[f1_scores == max(f1_scores)][0] \n",
    "        self.metric_cutoff_dict[\"f1_scores\"] = {\"map\":cutoff_map,\n",
    "                                                \"optimum\": cutoff_optimum}         \n",
    "        \n",
    "        cutoff_map = list(zip(cutoffs, precision_scores))\n",
    "        with open(os.path.join(self.output_path, \n",
    "                               \"%s_precision_cutoff_map.json\"%self.algorithm_name), \n",
    "                  \"w\") as outfile:\n",
    "            json.dump(cutoff_map, outfile)  \n",
    "        cutoff_optimum = cutoffs[precision_scores == max(precision_scores)][0]  \n",
    "        self.metric_cutoff_dict[\"precision_scores\"] = {\"map\":cutoff_map, \n",
    "                                                      \"optimum\": cutoff_optimum}         \n",
    "        \n",
    "        cutoff_map = list(zip(cutoffs, recall_scores))\n",
    "        with open(os.path.join(self.output_path, \n",
    "                               \"%s_recall_cutoff_map.json\"%self.algorithm_name), \n",
    "                  \"w\") as outfile:\n",
    "            json.dump(cutoff_map, outfile)  \n",
    "        cutoff_optimum = cutoffs[recall_scores == max(recall_scores)][0]  \n",
    "        self.metric_cutoff_dict[\"recall_scores\"] = {\"map\":cutoff_map, \n",
    "                                                    \"optimum\": cutoff_optimum}     \n",
    "        \n",
    "        with open(os.path.join(self.output_path, \n",
    "                               \"%s_metric_cutoff_dict.json\"%self.algorithm_name), \n",
    "                  \"w\") as outfile:\n",
    "            json.dump(self.metric_cutoff_dict, outfile)         \n",
    "       \n",
    "    def prediction_hardclasses_metrics(self, \n",
    "                                       x_data, \n",
    "                                       y_data, \n",
    "                                       metric_cutoff_type = \"accuracy_scores\"):\n",
    "        '''\n",
    "        Predict hardclasses based on metric cutoff.\n",
    "        Save metrics output.\n",
    "        '''\n",
    "        predicted = self.get_predictions(x_data)\n",
    "        pred_hardclasses = (predicted>self.metric_cutoff_dict[metric_cutoff_type][\"optimum\"]).astype(int)\n",
    "        np.save(os.path.join(self.output_path, \n",
    "                               \"%s_pred_hardclasses.npy\"%self.algorithm_name), \n",
    "                pred_hardclasses)  \n",
    "        \n",
    "        fbeta =fbeta_score(y_data, \n",
    "                           pred_hardclasses,\n",
    "                           beta=2)\n",
    "        cm = confusion_matrix(y_data, \n",
    "                             pred_hardclasses)\n",
    "        class_report = classification_report(y_data, \n",
    "                                            pred_hardclasses)  \n",
    "        roc_auc = roc_auc_score(y_data, \n",
    "                                pred_hardclasses)  \n",
    "        accuracy = accuracy_score(y_data, \n",
    "                                  pred_hardclasses)\n",
    "        f1score = f1_score(y_data, \n",
    "                           pred_hardclasses)\n",
    "        precision = precision_score(y_data,\n",
    "                                    pred_hardclasses)\n",
    "        recall = recall_score(y_data,\n",
    "                              pred_hardclasses)\n",
    "        \n",
    "        # AUROC represents the likelihood of the model distinguishing observations from two classes.\n",
    "        return fbeta, roc_auc, cm, class_report, accuracy, f1score, precision, recall\n",
    "    \n",
    "    def call_metric(self, \n",
    "                    x_train, \n",
    "                    y_train, \n",
    "                    x_val, \n",
    "                    y_val, \n",
    "                    x_test, \n",
    "                    y_test):\n",
    "        fbeta, roc_auc, cm, class_report, accuracy, f1score, precision, recall = self.prediction_hardclasses_metrics(x_train,\n",
    "                                                                                                                     y_train)\n",
    "        self.store_metric(fbeta, \n",
    "                          roc_auc, \n",
    "                          cm, \n",
    "                          class_report, \n",
    "                          accuracy, \n",
    "                          f1score, \n",
    "                          precision, \n",
    "                          recall,\n",
    "                          \"train_data\")\n",
    "        \n",
    "        fbeta, roc_auc, cm, class_report, accuracy, f1score, precision, recall = self.prediction_hardclasses_metrics(x_val, \n",
    "                                                                                                                     y_val) \n",
    "        self.store_metric(fbeta, \n",
    "                          roc_auc, \n",
    "                          cm, \n",
    "                          class_report, \n",
    "                          accuracy, \n",
    "                          f1score, \n",
    "                          precision, \n",
    "                          recall, \n",
    "                          \"validation_data\")\n",
    "        \n",
    "        fbeta, roc_auc, cm, class_report, accuracy, f1score, precision, recall = self.prediction_hardclasses_metrics(x_test,\n",
    "                                                                                                                     y_test)\n",
    "        self.store_metric(fbeta, \n",
    "                          roc_auc, \n",
    "                          cm, \n",
    "                          class_report, \n",
    "                          accuracy, \n",
    "                          f1score, \n",
    "                          precision, \n",
    "                          recall, \n",
    "                          \"test_data\")\n",
    "        \n",
    "        with open(os.path.join(self.output_path, \n",
    "                               \"%s_metrics.json\"%self.algorithm_name), \n",
    "                  \"w\") as outfile:\n",
    "            json.dump(self.metric, outfile)  \n",
    "            \n",
    "    def store_metric(self, \n",
    "                    fbeta, \n",
    "                    roc_auc, \n",
    "                    cm, \n",
    "                    class_report, \n",
    "                    accuracy, \n",
    "                    f1score, \n",
    "                    precision, \n",
    "                    recall,\n",
    "                     key):\n",
    "        self.metric[key] = {}        \n",
    "        self.metric[key][\"fbeta\"] = fbeta\n",
    "        self.metric[key][\"roc_auc_score\"] = roc_auc\n",
    "        self.metric[key][\"confusion_matrix\"] = cm.tolist()   \n",
    "        self.metric[key][\"accuracy\"] = accuracy   \n",
    "        self.metric[key][\"f1score\"] = f1score   \n",
    "        self.metric[key][\"precision\"] = precision   \n",
    "        self.metric[key][\"recall\"] = recall \n",
    "        \n",
    "        with open(os.path.join(self.output_path, \n",
    "                               \"%s_classification_report.txt\"%self.algorithm_name),\n",
    "                  \"a+\") as outfile:\n",
    "            print(class_report, file=outfile)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calling classification_pred_and_metric_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = classification_pred_and_metric_profile(model, \n",
    "                                                \"rf\")\n",
    "output.get_metric_cutoff(x_train, \n",
    "                         y_train)\n",
    "output.call_metric(x_train, \n",
    "                   y_train, \n",
    "                   x_val, \n",
    "                   y_val, \n",
    "                   x_test, \n",
    "                   y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression_pred_and_metric_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regression_pred_and_metric_profile:\n",
    "    '''\n",
    "    Predicts the hardclasses for classiciation model.\n",
    "    Uses the cut-off based on selection of metric.\n",
    "    Stores and saves metrics.\n",
    "    '''\n",
    "    def __init__(self, model, algorithm_name):\n",
    "        self.model = model\n",
    "        self.metric = {}\n",
    "        self.algorithm_name = algorithm_name\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.output_path = os.path.join(\"../output\",timestr)\n",
    "        os.makedirs(self.output_path) \n",
    "    \n",
    "    def get_predictions(self, \n",
    "                        data):\n",
    "        '''\n",
    "        Predict probabilities for data.\n",
    "        '''\n",
    "        return self.model.predict(data)       \n",
    "       \n",
    "    def prediction_and_metrics(self, \n",
    "                               x_data, \n",
    "                               y_data):\n",
    "        '''\n",
    "        Predict hardclasses based on metric cutoff.\n",
    "        Save metrics output.\n",
    "        '''\n",
    "        predicted = self.get_predictions(x_data)\n",
    "        np.save(os.path.join(self.output_path, \n",
    "                               \"%s_predicted.npy\"%self.algorithm_name),\n",
    "                predicted)  \n",
    "        \n",
    "        mae = mean_absolute_error(y_data, \n",
    "                                  predicted)\n",
    "        mse = mean_squared_error(y_data,\n",
    "                                 predicted,\n",
    "                                 squared=True)\n",
    "        slope, _, r_value, p_value, std_err = scipy.stats.mstats.linregress(y_data, \n",
    "                                                                            predicted)\n",
    "        r_squared = r2_score(y_data, \n",
    "                             predicted)        \n",
    "        \n",
    "        # AUROC represents the likelihood of the model distinguishing observations from two classes.\n",
    "        return mae, mse, slope, r_value, p_value, std_err, r_squared\n",
    "    \n",
    "    def call_metric(self, \n",
    "                    x_train, \n",
    "                    y_train, \n",
    "                    x_val, \n",
    "                    y_val, \n",
    "                    x_test, \n",
    "                    y_test):\n",
    "        mae, mse, slope, r_value, p_value, std_err, r_squared = self.prediction_and_metrics(x_train,\n",
    "                                                                                                    y_train)\n",
    "        self.store_metric(mae, \n",
    "                          mse, \n",
    "                          slope, \n",
    "                          r_value, \n",
    "                          p_value, \n",
    "                          std_err, \n",
    "                          r_squared,\n",
    "                          \"train_data\")\n",
    "        \n",
    "        mae, mse, slope, r_value, p_value, std_err, r_squared = self.prediction_and_metrics(x_val,\n",
    "                                                                                            y_val) \n",
    "        self.store_metric(mae, \n",
    "                          mse, \n",
    "                          slope, \n",
    "                          r_value, \n",
    "                          p_value, \n",
    "                          std_err, \n",
    "                          r_squared, \n",
    "                          \"validation_data\")\n",
    "        \n",
    "        mae, mse, slope, r_value, p_value, std_err, r_squared = self.prediction_and_metrics(x_test,\n",
    "                                                                                            y_test)\n",
    "        self.store_metric(mae, \n",
    "                          mse, \n",
    "                          slope, \n",
    "                          r_value, \n",
    "                          p_value, \n",
    "                          std_err, \n",
    "                          r_squared,\n",
    "                          \"test_data\")\n",
    "        \n",
    "        with open(os.path.join(self.output_path, \n",
    "                               \"%s_metrics.json\"%self.algorithm_name), \n",
    "                  \"w\") as outfile:\n",
    "            json.dump(self.metric, outfile)  \n",
    "            \n",
    "    def store_metric(self, \n",
    "                    mae, \n",
    "                    mse, \n",
    "                    slope, \n",
    "                    r_value, \n",
    "                    p_value, \n",
    "                    std_err, \n",
    "                    r_squared,\n",
    "                    key):\n",
    "        self.metric[key] = {}        \n",
    "        self.metric[key][\"mean_absolute_error\"] = mae\n",
    "        self.metric[key][\"root_mean_squared_error\"] = mse\n",
    "        self.metric[key][\"slope\"] = slope   \n",
    "        self.metric[key][\"r_value\"] = r_value   \n",
    "        self.metric[key][\"p_value\"] = p_value   \n",
    "        self.metric[key][\"std_err\"] = std_err   \n",
    "        self.metric[key][\"r_squared\"] = r_squared  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calling regression_pred_and_metric_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = regression_pred_and_metric_profile(model,\"rf\")\n",
    "output.call_metric(x_train, \n",
    "                   y_train, \n",
    "                   x_val, \n",
    "                   y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
